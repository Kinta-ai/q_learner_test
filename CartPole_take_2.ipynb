{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NetworkCopier():\n",
    "    def __init__(self, estimator,target):\n",
    "        est_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(estimator.scope)]\n",
    "        est_params = sorted(est_params, key = lambda x: x.name)\n",
    "\n",
    "        tar_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(target.scope)]\n",
    "        tar_params = sorted(tar_params, key = lambda x: x.name)\n",
    "\n",
    "        self.update_ops = [tar_var.assign(est_var) for est_var,tar_var in zip(est_params,tar_params)]\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def copy_and_freeze(self,sess): \n",
    "        sess.run(self.update_ops)\n",
    "        return\n",
    "        \n",
    "        \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 50000):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def add_new(self, state, action, reward, next_state, done):\n",
    "        entry = (state,action,reward,next_state,done)\n",
    "        self.buffer.append(entry)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def batch(self, n = 100):\n",
    "        if len(self.buffer) < n:\n",
    "            minibatch = 0\n",
    "        else:\n",
    "            minibatch = random.sample(self.buffer, n)\n",
    "        return minibatch\n",
    "\n",
    "\n",
    "class Q_nn():\n",
    "    def __init__(self, scope = 'default'):\n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(scope):\n",
    "            self.__build_model()\n",
    "    \n",
    "    def __build_model(self):\n",
    "         # state\n",
    "        self.X_states = tf.placeholder(shape = [None, 4], dtype = tf.float32)\n",
    "        \n",
    "        # target values (R+maxQ)\n",
    "        self.Q_targets = tf.placeholder(shape = [None,2], dtype = tf.float32)\n",
    "\n",
    "        self.dense1 = tf.layers.dense(inputs = self.X_states, units=12, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.layers.dense(self.dense1,12, activation = tf.nn.relu)\n",
    "        self.dense3 = tf.layers.dense(self.dense2,12, activation = tf.nn.relu)\n",
    "        self.Q_est = tf.layers.dense(self.dense3,2)\n",
    "                       \n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_targets, self.Q_est)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "    \n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        return sess.run(self.Q_est, { self.X_states: state })\n",
    "        \n",
    "        \n",
    "    def update(self, sess, states, targets):\n",
    "        feed_dict = { self.X_states: states, self.Q_targets: targets }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Set\n"
     ]
    }
   ],
   "source": [
    "episodes = 2000\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "batch_size = 32\n",
    "discount = 0.95\n",
    "max_steps = 1000\n",
    "tf.reset_default_graph()\n",
    "\n",
    "Q_estimator = Q_nn(scope = 'estimator')\n",
    "Q_target = Q_nn(scope = 'target')\n",
    "Freezer = NetworkCopier(Q_estimator,Q_target)\n",
    "Buffer = ReplayBuffer(50000)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "env = gym.make('CartPole-v0')\n",
    "print('Graph Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, loss = 0.5737829208374023, total steps = 14.0\n",
      "Episode 50, loss = 0.4268091320991516, total steps = 11.0\n",
      "Episode 75, loss = 0.45615994930267334, total steps = 29.0\n",
      "Episode 100, loss = 0.42977437376976013, total steps = 11.0\n",
      "Episode 125, loss = 0.5267625451087952, total steps = 11.0\n",
      "Episode 150, loss = 0.18501035869121552, total steps = 11.0\n",
      "Episode 175, loss = 0.5742748379707336, total steps = 13.0\n",
      "Episode 200, loss = 0.8848893642425537, total steps = 16.0\n",
      "Episode 225, loss = 0.7866421341896057, total steps = 10.0\n",
      "Episode 250, loss = 2.9266304969787598, total steps = 9.0\n",
      "Episode 275, loss = 0.7706742286682129, total steps = 11.0\n",
      "Episode 300, loss = 2.1241536140441895, total steps = 8.0\n",
      "Episode 325, loss = 2.170259714126587, total steps = 10.0\n",
      "Episode 350, loss = 0.7667553424835205, total steps = 10.0\n",
      "Episode 375, loss = 1.150133490562439, total steps = 10.0\n",
      "Episode 400, loss = 2.88472843170166, total steps = 10.0\n",
      "Episode 425, loss = 0.19885188341140747, total steps = 9.0\n",
      "Episode 450, loss = 2.047039747238159, total steps = 49.0\n",
      "Episode 475, loss = 1.595274567604065, total steps = 42.0\n",
      "Episode 500, loss = 1.1185619831085205, total steps = 9.0\n",
      "Episode 525, loss = 0.9275786280632019, total steps = 9.0\n",
      "Episode 550, loss = 1.9179110527038574, total steps = 10.0\n",
      "Episode 575, loss = 0.25130027532577515, total steps = 158.0\n",
      "Episode 600, loss = 0.19139355421066284, total steps = 129.0\n",
      "Episode 625, loss = 0.5814619660377502, total steps = 49.0\n",
      "Episode 650, loss = 0.2626263201236725, total steps = 62.0\n",
      "Episode 675, loss = 2.2434332370758057, total steps = 97.0\n",
      "Episode 700, loss = 0.611030638217926, total steps = 82.0\n",
      "Episode 725, loss = 0.13203668594360352, total steps = 77.0\n",
      "Episode 750, loss = 2.1780731678009033, total steps = 77.0\n",
      "Episode 775, loss = 0.12050339579582214, total steps = 118.0\n",
      "Episode 800, loss = 0.07649866491556168, total steps = 44.0\n",
      "Episode 825, loss = 0.2580628991127014, total steps = 92.0\n",
      "Episode 850, loss = 0.0732058435678482, total steps = 89.0\n",
      "Episode 875, loss = 0.05934927612543106, total steps = 94.0\n",
      "Episode 900, loss = 0.10282759368419647, total steps = 52.0\n",
      "Episode 925, loss = 1.731024146080017, total steps = 200.0\n",
      "Episode 950, loss = 0.10235056281089783, total steps = 95.0\n",
      "Episode 975, loss = 0.26313716173171997, total steps = 108.0\n",
      "Episode 1000, loss = 1.2016658782958984, total steps = 137.0\n",
      "Episode 1025, loss = 0.17294099926948547, total steps = 71.0\n",
      "Episode 1050, loss = 4.067443370819092, total steps = 96.0\n",
      "Episode 1075, loss = 0.04790511354804039, total steps = 101.0\n",
      "Episode 1100, loss = 0.10720600187778473, total steps = 200.0\n",
      "Episode 1125, loss = 0.06278933584690094, total steps = 112.0\n",
      "Episode 1150, loss = 0.06231207400560379, total steps = 86.0\n",
      "Episode 1175, loss = 0.06944967806339264, total steps = 83.0\n",
      "Episode 1200, loss = 0.06143832951784134, total steps = 86.0\n",
      "Episode 1225, loss = 2.2754716873168945, total steps = 192.0\n",
      "Episode 1250, loss = 1.0804541110992432, total steps = 137.0\n",
      "Episode 1275, loss = 0.044938310980796814, total steps = 99.0\n",
      "Episode 1300, loss = 0.02543480694293976, total steps = 95.0\n",
      "Episode 1325, loss = 0.03708228841423988, total steps = 200.0\n",
      "Episode 1350, loss = 0.05827456712722778, total steps = 134.0\n",
      "Episode 1375, loss = 0.0416528582572937, total steps = 94.0\n",
      "Episode 1400, loss = 0.0614502876996994, total steps = 142.0\n",
      "Episode 1425, loss = 0.025166697800159454, total steps = 119.0\n",
      "Episode 1450, loss = 0.5935042500495911, total steps = 185.0\n",
      "Episode 1475, loss = 0.026206497102975845, total steps = 151.0\n",
      "Episode 1500, loss = 0.5881792306900024, total steps = 138.0\n",
      "Episode 1525, loss = 0.030847545713186264, total steps = 128.0\n",
      "Episode 1550, loss = 0.04103205353021622, total steps = 92.0\n",
      "Episode 1575, loss = 0.026411965489387512, total steps = 144.0\n",
      "Episode 1600, loss = 0.06538059562444687, total steps = 108.0\n",
      "Episode 1625, loss = 0.04380722716450691, total steps = 141.0\n",
      "Episode 1650, loss = 0.028173765167593956, total steps = 172.0\n",
      "Episode 1675, loss = 0.03142989054322243, total steps = 156.0\n",
      "Episode 1700, loss = 0.03203658014535904, total steps = 121.0\n",
      "Episode 1725, loss = 0.02035442739725113, total steps = 200.0\n",
      "Episode 1750, loss = 0.9005765318870544, total steps = 88.0\n",
      "Episode 1775, loss = 0.04682132974267006, total steps = 84.0\n",
      "Episode 1800, loss = 0.033652257174253464, total steps = 140.0\n",
      "Episode 1825, loss = 0.0385005921125412, total steps = 112.0\n",
      "Episode 1850, loss = 0.04540625587105751, total steps = 126.0\n",
      "Episode 1875, loss = 0.013296612538397312, total steps = 200.0\n",
      "Episode 1900, loss = 0.047208983451128006, total steps = 200.0\n",
      "Episode 1925, loss = 0.017900565639138222, total steps = 123.0\n",
      "Episode 1950, loss = 0.04886169359087944, total steps = 78.0\n",
      "Episode 1975, loss = 0.04014614596962929, total steps = 79.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(episodes):\n",
    "    epsilon = max(epsilon_decay**e, epsilon_min)\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        curr_state = observation\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            curr_state_reshape = np.reshape(curr_state,(1,4))\n",
    "            q_est = Q_estimator.predict(sess,curr_state_reshape)[0]\n",
    "            action = np.argmax(q_est)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_state = observation\n",
    "        Buffer.add_new(curr_state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    minibatch = Buffer.batch(batch_size)\n",
    "    if minibatch:\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_reshape = np.reshape(state, (1,4))\n",
    "            next_state_reshape = np.reshape(next_state, (1,4))            \n",
    "            target = Q_estimator.predict(sess,state_reshape)[0]\n",
    "            q_max = np.amax(Q_target.predict(sess,next_state_reshape)[0])\n",
    "\n",
    "            # this one does well\n",
    "            target[action] = reward\n",
    "            if not done:\n",
    "                target[action] += discount*q_max\n",
    "                \n",
    "            # this one does not\n",
    "            # target[action] = reward + discount*q_max\n",
    "            \n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "        state_array = np.stack(states)\n",
    "        target_array = np.stack(targets)\n",
    "        loss = Q_estimator.update(sess, state_array, target_array)\n",
    "        \n",
    "    if e%25 == 0 and e > 0:\n",
    "        Freezer.copy_and_freeze(sess)\n",
    "        print('Episode {}, loss = {}, total steps = {}'.format(e,loss,total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luckiest of 100 random agents survives 79 time steps\n",
      "Best of 100 learned agent survives 200 time steps\n",
      "CPU times: user 7.88 s, sys: 344 ms, total: 8.22 s\n",
      "Wall time: 8.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random_results = []\n",
    "agent_results = []\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    random_results.append(total_steps)    \n",
    "\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        state_reshape = np.reshape(observation,(1,4))\n",
    "        q_est = Q_estimator.predict(sess,state_reshape)[0]\n",
    "    #     print(q_est)\n",
    "        action = np.argmax(q_est)\n",
    "    #     print(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    agent_results.append(total_steps)\n",
    "# env.render(close=True)\n",
    "print('Luckiest of 100 random agents survives {} time steps'.format(max(random_results)))\n",
    "print('Best of 100 learned agent survives {} time steps'.format(max(agent_results)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
