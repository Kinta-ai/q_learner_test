{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkCopier():\n",
    "    def __init__(self, estimator,target):\n",
    "        est_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(estimator.scope)]\n",
    "        est_params = sorted(est_params, key = lambda x: x.name)\n",
    "\n",
    "        tar_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(target.scope)]\n",
    "        tar_params = sorted(tar_params, key = lambda x: x.name)\n",
    "\n",
    "        self.update_ops = [tar_var.assign(est_var) for est_var,tar_var in zip(est_params,tar_params)]\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def copy_and_freeze(self,sess): \n",
    "        sess.run(self.update_ops)\n",
    "        return\n",
    "        \n",
    "        \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 50000):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def add_new(self, state, action, reward, next_state, done):\n",
    "        entry = (state,action,reward,next_state,done)\n",
    "        self.buffer.append(entry)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def batch(self, n = 100):\n",
    "        if len(self.buffer) < n:\n",
    "            minibatch = 0\n",
    "        else:\n",
    "            minibatch = random.sample(self.buffer, n)\n",
    "        return minibatch\n",
    "\n",
    "\n",
    "class Q_learner():\n",
    "    def __init__(self, state_size, action_size, lr = 0.001, scope='default'):\n",
    "        self.scope = scope\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def setup(self):\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.__set_placeholders(self.state_size, self.action_size)\n",
    "            self.__build_model()\n",
    "            self.__set_loss_and_opt(self.lr)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __set_placeholders(self, state_size, action_size):\n",
    "                # state\n",
    "        self.X_states = tf.placeholder(shape = [None, self.state_size], dtype = tf.float32)\n",
    "        \n",
    "        # target values (R+maxQ)\n",
    "        self.Q_targets = tf.placeholder(shape = [None, self.action_size], dtype = tf.float32)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __build_model(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __set_loss_and_opt(self, lr = 0.001):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        return sess.run(self.Q_est, { self.X_states: state })\n",
    "        \n",
    "        \n",
    "    def update(self, sess, states, targets):\n",
    "        feed_dict = { self.X_states: states, self.Q_targets: targets }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class Q_nn(Q_learner):\n",
    "# class Q_nn():\n",
    "#     def __init__(self, state_size, action_size, lr = 0.001, scope='default'):\n",
    "#         self.scope = scope\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.lr = lr\n",
    "        \n",
    "#         return\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def setup(self):\n",
    "#         with tf.variable_scope(self.scope):\n",
    "#             self.__set_placeholders(self.state_size, self.action_size)\n",
    "#             self.__build_model()\n",
    "#             self.__set_loss_and_opt(self.lr)\n",
    "            \n",
    "#         return\n",
    "    \n",
    "    \n",
    "#     def __set_placeholders(self, state_size, action_size):\n",
    "#                 # state\n",
    "#         self.X_states = tf.placeholder(shape = [None, self.state_size], dtype = tf.float32)\n",
    "        \n",
    "#         # target values (R+maxQ)\n",
    "#         self.Q_targets = tf.placeholder(shape = [None, self.action_size], dtype = tf.float32)\n",
    "        \n",
    "#         return\n",
    "    \n",
    "    \n",
    "    def __build_model(self):\n",
    "\n",
    "        \n",
    "        self.dense1 = tf.layers.dense(inputs = self.X_states, units=12, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.layers.dense(self.dense1,12, activation = tf.nn.relu)\n",
    "        self.dense3 = tf.layers.dense(self.dense2,12, activation = tf.nn.relu)\n",
    "        self.Q_est = tf.layers.dense(self.dense3,2)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __set_loss_and_opt(self, lr):\n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_targets, self.Q_est)\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "#     def predict(self, sess, state):\n",
    "#         return sess.run(self.Q_est, { self.X_states: state })\n",
    "     \n",
    "    \n",
    "#     def update(self, sess, states, targets):\n",
    "#         feed_dict = { self.X_states: states, self.Q_targets: targets }\n",
    "#         _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c1a58f951878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CartPole-v1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1216\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1249\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "episodes = 2000\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "batch_size = 32\n",
    "discount = 0.95\n",
    "max_steps = 1000\n",
    "tf.reset_default_graph()\n",
    "\n",
    "Q_estimator = Q_nn(4,2,0.001,scope = 'estimator')\n",
    "Q_target = Q_nn(4,2,0.001,scope = 'target')\n",
    "Q_estimator.setup()\n",
    "Q_target.setup()\n",
    "Freezer = NetworkCopier(Q_estimator,Q_target)\n",
    "Buffer = ReplayBuffer(50000)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "env = gym.make('CartPole-v1')\n",
    "print('Graph Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Q_nn' object has no attribute 'Q_est'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f78465cef747>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mstate_reshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mnext_state_reshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mq_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-14ff71a9545a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, sess, state)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Q_nn' object has no attribute 'Q_est'"
     ]
    }
   ],
   "source": [
    "for e in range(episodes):\n",
    "    epsilon = max(epsilon_decay**e, epsilon_min)\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    loss = None\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        curr_state = observation\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            curr_state_reshape = np.reshape(curr_state,(1,4))\n",
    "            q_est = Q_estimator.predict(sess,curr_state_reshape)[0]\n",
    "            action = np.argmax(q_est)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_state = observation\n",
    "        Buffer.add_new(curr_state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    for i in range(10):\n",
    "        minibatch = Buffer.batch(batch_size)\n",
    "        if minibatch:\n",
    "            states = []\n",
    "            targets = []\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                state_reshape = np.reshape(state, (1,4))\n",
    "                next_state_reshape = np.reshape(next_state, (1,4))            \n",
    "                target = Q_estimator.predict(sess,state_reshape)[0]\n",
    "                q_max = np.amax(Q_target.predict(sess,next_state_reshape)[0])\n",
    "\n",
    "                target[action] = reward\n",
    "                if not done:\n",
    "                    target[action] += discount*q_max\n",
    "\n",
    "                states.append(state)\n",
    "                targets.append(target)\n",
    "            state_array = np.stack(states)\n",
    "            target_array = np.stack(targets)\n",
    "            loss = Q_estimator.update(sess, state_array, target_array)\n",
    "        \n",
    "    if loss:\n",
    "        Freezer.copy_and_freeze(sess)\n",
    "        print('Episode {}, loss = {}, total steps = {}'.format(e,loss,total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_results = []\n",
    "agent_results = []\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    random_results.append(total_steps)    \n",
    "\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        state_reshape = np.reshape(observation,(1,4))\n",
    "        q_est = Q_estimator.predict(sess,state_reshape)[0]\n",
    "    #     print(q_est)\n",
    "        action = np.argmax(q_est)\n",
    "    #     print(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    agent_results.append(total_steps)\n",
    "# env.render(close=True)\n",
    "print('Luckiest of 100 random agents survives {} time steps'.format(max(random_results)))\n",
    "print('Best of 100 learned agent survives {} time steps'.format(max(agent_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3 (airl)",
   "language": "python",
   "name": "airl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
