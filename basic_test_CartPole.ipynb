{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_nn():\n",
    "    \n",
    "    def __init__(self, scope = 'estimator'):\n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(scope):\n",
    "            self.__build_graph()\n",
    "            \n",
    "\n",
    "    def __build_graph(self):\n",
    "        # state matrix\n",
    "        self.X_states = tf.placeholder(shape = [None, 4], dtype = tf.float32)\n",
    "        \n",
    "        # target values (R+maxQ)\n",
    "        self.Q_targets = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "            \n",
    "        # action as an index\n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        \n",
    "        self.batch_size = self.X_states.shape[0]\n",
    "        self.action_size = 2\n",
    "        self.action_one_hots = tf.one_hot(self.actions,self.action_size,axis = -1)\n",
    "        \n",
    "        self.W_fc1 = tf.Variable(tf.truncated_normal([4,32],0.1))\n",
    "        self.b_fc1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "        self.fc1 = tf.matmul(self.X_states,self.W_fc1) + self.b_fc1\n",
    "        \n",
    "        self.relu1 = tf.nn.relu(self.fc1)\n",
    "        \n",
    "        self.W_fc2 = tf.Variable(tf.truncated_normal([32,2],0.1))\n",
    "        self.b_fc2 = tf.Variable(tf.constant(0.1,shape=[2]))\n",
    "        \n",
    "        self.Q_est = tf.matmul(self.relu1,self.W_fc2) + self.b_fc2\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_Qs = tf.reduce_sum(tf.multiply(self.Q_est,self.action_one_hots),-1)\n",
    "               \n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_targets, self.action_Qs)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "\n",
    "    def predict(self, sess, state):\n",
    "        return sess.run(self.Q_est, { self.X_states: state })\n",
    "        \n",
    "        \n",
    "    def update(self, sess, states, actions, targets):\n",
    "        feed_dict = { self.X_states: states, self.Q_targets: targets, self.actions: actions}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class NetworkCopier():\n",
    "    def __init__(self, estimator,target):\n",
    "        est_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(estimator.scope)]\n",
    "        est_params = sorted(est_params, key = lambda x: x.name)\n",
    "\n",
    "        tar_params = [variable for variable in tf.trainable_variables() if variable.name.startswith(target.scope)]\n",
    "        tar_params = sorted(tar_params, key = lambda x: x.name)\n",
    "\n",
    "        self.update_ops = [tar_var.assign(est_var) for est_var,tar_var in zip(est_params,tar_params)]\n",
    "    \n",
    "    \n",
    "    def copy_and_freeze(self,sess): \n",
    "        sess.run(self.update_ops)\n",
    "        \n",
    "        \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "        \n",
    "    def add_new(self, state, action, reward, next_state):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            _ = self.buffer.pop()\n",
    "        entry = (state,action,reward,next_state)\n",
    "        self.buffer.append(entry)\n",
    "        \n",
    "        \n",
    "    def batch(self, n = 100):\n",
    "        if len(self.buffer) < n:\n",
    "            return self.buffer\n",
    "        else:\n",
    "            return random.sample(self.buffer, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-21 17:24:09,261] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Set\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "explore_steps = int(episodes*0.25) # how many episodes to do pure exploration\n",
    "iterations = 10 # per episode\n",
    "max_steps = 1000 # per episode\n",
    "batch_size = 100\n",
    "discount = 0.99\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "Q_estimator = Q_nn(scope = 'estimator')\n",
    "Q_target = Q_nn(scope = 'target')\n",
    "Freezer = NetworkCopier(Q_estimator,Q_target)\n",
    "Buffer = ReplayBuffer(2500)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "env = gym.make('CartPole-v0')\n",
    "print('Graph Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, loss: 36.20223426818848, total steps = 13.0\n",
      "Episode 25, loss: 0.5808361381292343, total steps = 75.0\n",
      "Episode 50, loss: 0.31379234790802, total steps = 11.0\n",
      "Episode 75, loss: 0.2676816999912262, total steps = 20.0\n",
      "Episode 100, loss: 0.2788293197751045, total steps = 15.0\n",
      "Episode 125, loss: 0.2887959197163582, total steps = 11.0\n",
      "Episode 150, loss: 0.28190267384052276, total steps = 12.0\n",
      "Episode 175, loss: 0.3077401012182236, total steps = 16.0\n",
      "Episode 200, loss: 0.32511214911937714, total steps = 23.0\n",
      "Episode 225, loss: 0.32575283050537107, total steps = 15.0\n",
      "Episode 250, loss: 0.3532680481672287, total steps = 25.0\n",
      "Episode 275, loss: 0.4300054043531418, total steps = 10.0\n",
      "Episode 300, loss: 0.5083339989185334, total steps = 29.0\n",
      "Episode 325, loss: 0.6105148643255234, total steps = 48.0\n",
      "Episode 350, loss: 0.8126034796237945, total steps = 63.0\n",
      "Episode 375, loss: 0.9641392529010773, total steps = 60.0\n",
      "Episode 400, loss: 1.3265783131122588, total steps = 37.0\n",
      "Episode 425, loss: 1.6267268657684326, total steps = 39.0\n",
      "Episode 450, loss: 2.222074103355408, total steps = 48.0\n",
      "Episode 475, loss: 2.9109992504119875, total steps = 91.0\n",
      "Episode 500, loss: 3.5769615650177, total steps = 66.0\n",
      "Episode 525, loss: 4.5539827108383175, total steps = 41.0\n",
      "Episode 550, loss: 5.252020597457886, total steps = 49.0\n",
      "Episode 575, loss: 6.900196933746338, total steps = 37.0\n",
      "Episode 600, loss: 8.349427938461304, total steps = 82.0\n",
      "Episode 625, loss: 8.775631427764893, total steps = 46.0\n",
      "Episode 650, loss: 9.686930418014526, total steps = 48.0\n",
      "Episode 675, loss: 11.115619468688966, total steps = 44.0\n",
      "Episode 700, loss: 10.690672492980957, total steps = 50.0\n",
      "Episode 725, loss: 11.178985404968262, total steps = 51.0\n",
      "Episode 750, loss: 10.938673114776611, total steps = 62.0\n",
      "Episode 775, loss: 11.499748706817627, total steps = 71.0\n",
      "Episode 800, loss: 12.387324047088622, total steps = 48.0\n",
      "Episode 825, loss: 12.42400770187378, total steps = 82.0\n",
      "Episode 850, loss: 13.859279155731201, total steps = 54.0\n",
      "Episode 875, loss: 14.652147960662841, total steps = 54.0\n",
      "Episode 900, loss: 16.717030620574953, total steps = 57.0\n",
      "Episode 925, loss: 17.786117267608642, total steps = 88.0\n",
      "Episode 950, loss: 18.469060230255128, total steps = 46.0\n",
      "Episode 975, loss: 24.26016902923584, total steps = 72.0\n",
      "Wall time: 19min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in range(episodes):\n",
    "    cumulative_loss = 0\n",
    "    if e < explore_steps: # pure exploration\n",
    "        epsilon = 1\n",
    "    else: # epsilon-greedy exploration/exploitation\n",
    "        epsilon = max(np.exp((e-explore_steps)/-50),0.1)\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        curr_state = observation\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            curr_state_reshape = np.reshape(curr_state,(1,4))\n",
    "            q_est = Q_estimator.predict(sess,curr_state_reshape)\n",
    "            action = np.argmin(q_est)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_state = observation\n",
    "        Buffer.add_new(curr_state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    for i in range(iterations):\n",
    "        states, actions, rewards, next_states = zip(*Buffer.batch(batch_size))\n",
    "        targets = []\n",
    "        for reward, next_state in zip(rewards,next_states):\n",
    "            next_state_reshape = np.reshape(next_state, (1,4))\n",
    "            q_max = np.max(Q_target.predict(sess,next_state_reshape))\n",
    "            target = reward + discount*q_max\n",
    "            targets.append(target)\n",
    "        state_array = np.stack(states,axis=0)\n",
    "        action_array = np.array(actions)\n",
    "        target_array = np.array(targets)\n",
    "        loss = Q_estimator.update(sess, state_array,action_array,target_array)\n",
    "        cumulative_loss += loss\n",
    "    if e%25 == 0:\n",
    "        Freezer.copy_and_freeze(sess)\n",
    "        print('Episode {}, loss: {}, total steps = {}'.format(e,cumulative_loss/iterations,total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_current'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a32815966fdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"random_results = []\\nagent_results = []\\nfor t in range(100):\\n    done = False\\n    observation = env.reset()\\n    total_steps = 0\\n    while not done:\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        total_steps += 1\\n    random_results.append(total_steps)    \\n\\nfor t in range(100):\\n    done = False\\n    observation = env.reset()\\n    total_steps = 0\\n    while not done:\\n        env.render()\\n        state_reshape = np.reshape(observation,(1,4))\\n        q_est = Q_estimator.predict(sess,state_reshape)\\n    #     print(q_est)\\n        action = np.argmin(q_est)\\n    #     print(action)\\n        observation, reward, done, info = env.step(action)\\n        total_steps += 1\\n    agent_results.append(total_steps)\\nenv.render(close=True)\\nprint('Luckiest of 100 random agents survives {} time steps'.format(max(random_results)))\\nprint('Best learned agent survives {} time steps'.format(max(agent_results)))\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mswitch_to\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_current'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random_results = []\n",
    "agent_results = []\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    random_results.append(total_steps)    \n",
    "\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state_reshape = np.reshape(observation,(1,4))\n",
    "        q_est = Q_estimator.predict(sess,state_reshape)\n",
    "    #     print(q_est)\n",
    "        action = np.argmin(q_est)\n",
    "    #     print(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    agent_results.append(total_steps)\n",
    "env.render(close=True)\n",
    "print('Luckiest of 100 random agents survives {} time steps'.format(max(random_results)))\n",
    "print('Best learned agent survives {} time steps'.format(max(agent_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_current'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-032cd3a25343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtotal_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mstate_reshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mq_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dudemonkeys\\openai\\gym\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\deeplearning\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mswitch_to\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_current'"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state_reshape = np.reshape(observation,(1,4))\n",
    "        q_est = Q_estimator.predict(sess,state_reshape)\n",
    "    #     print(q_est)\n",
    "        action = env.action_space.sample()\n",
    "    #     print(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    agent_results.append(total_steps)\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (airl)",
   "language": "python",
   "name": "airl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
