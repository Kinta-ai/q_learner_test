{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_nn():\n",
    "    \n",
    "    def __init__(self, scope = 'estimator'):\n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(scope):\n",
    "            self.__build_graph()\n",
    "            \n",
    "\n",
    "    def __build_graph(self):\n",
    "        # state matrix\n",
    "        self.X_states = tf.placeholder(shape = [None, 4], dtype = tf.float32)\n",
    "        \n",
    "        # target values (R+maxQ)\n",
    "        self.Q_targets = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "            \n",
    "        # action as an index\n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        \n",
    "        self.batch_size = self.X_states.shape[0]\n",
    "        self.action_size = 2\n",
    "        self.action_one_hots = tf.one_hot(self.actions,self.action_size,axis = -1)\n",
    "        \n",
    "        self.W_fc1 = tf.Variable(tf.truncated_normal([4,32],0.1))\n",
    "        self.b_fc1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "        self.fc1 = tf.matmul(self.X_states,self.W_fc1) + self.b_fc1\n",
    "        \n",
    "        self.relu1 = tf.nn.relu(self.fc1)\n",
    "        \n",
    "        self.W_fc2 = tf.Variable(tf.truncated_normal([32,2],0.1))\n",
    "        self.b_fc2 = tf.Variable(tf.constant(0.1,shape=[2]))\n",
    "        \n",
    "        self.Q_est = tf.matmul(self.relu1,self.W_fc2) + self.b_fc2\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_Qs = tf.reduce_sum(tf.multiply(self.Q_est,self.action_one_hots),-1)\n",
    "               \n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_targets, self.action_Qs)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "\n",
    "    def predict(self, sess, state):\n",
    "        return sess.run(self.Q_est, { self.X_states: state })\n",
    "        \n",
    "        \n",
    "    def update(self, sess, states, actions, targets):\n",
    "        feed_dict = { self.X_states: states, self.Q_targets: targets, self.actions: actions}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class NetworkCopier():\n",
    "    def __init__(self, estimator,target):\n",
    "        est_vars = [variable for variable in tf.trainable_variables() if variable.name.startswith(estimator.scope)]\n",
    "        est_vars = sorted(est_vars, key = lambda x: x.name)\n",
    "\n",
    "        tar_vars = [variable for variable in tf.trainable_variables() if variable.name.startswith(target.scope)]\n",
    "        tar_vars = sorted(tar_vars, key = lambda x: x.name)\n",
    "\n",
    "        self.update_ops = [tar_var.assign(est_var) for est_var,tar_var in zip(est_vars,tar_vars)]\n",
    "    \n",
    "    \n",
    "    def copy_and_freeze(self,sess): \n",
    "        sess.run(self.update_ops)\n",
    "        \n",
    "        \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "        \n",
    "    def add_new(self, state, action, reward, next_state):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            _ = self.buffer.pop()\n",
    "        entry = (state,action,reward,next_state)\n",
    "        self.buffer.append(entry)\n",
    "        \n",
    "        \n",
    "    def batch(self, n = 100):\n",
    "        if len(self.buffer) < n:\n",
    "            return self.buffer\n",
    "        else:\n",
    "            return random.sample(self.buffer, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-27 13:30:56,521] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Set\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "explore_steps = int(episodes*0.25) # how many episodes to do pure exploration\n",
    "iterations = 10 # per episode\n",
    "max_steps = 1000 # per episode\n",
    "batch_size = 100\n",
    "discount = 0.99\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "Q_estimator = Q_nn(scope = 'estimator')\n",
    "Q_target = Q_nn(scope = 'target')\n",
    "Freezer = NetworkCopier(Q_estimator,Q_target)\n",
    "Buffer = ReplayBuffer(2500)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "env = gym.make('CartPole-v0')\n",
    "print('Graph Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, loss: 19.87629795074463, total steps = 12.0\n",
      "Episode 25, loss: 0.5317947566509247, total steps = 57.0\n",
      "Episode 50, loss: 0.3819993257522583, total steps = 9.0\n",
      "Episode 75, loss: 0.7842229396104813, total steps = 35.0\n",
      "Episode 100, loss: 0.4830306738615036, total steps = 19.0\n",
      "Episode 125, loss: 0.44875511825084685, total steps = 28.0\n",
      "Episode 150, loss: 0.47208432853221893, total steps = 16.0\n",
      "Episode 175, loss: 0.4970659613609314, total steps = 24.0\n",
      "Episode 200, loss: 0.5723933339118957, total steps = 13.0\n",
      "Episode 225, loss: 0.6282437562942504, total steps = 28.0\n",
      "Episode 250, loss: 0.6839518189430237, total steps = 35.0\n",
      "Episode 275, loss: 0.7458848416805267, total steps = 18.0\n",
      "Episode 300, loss: 1.0102652251720428, total steps = 20.0\n",
      "Episode 325, loss: 1.1592188477516174, total steps = 19.0\n",
      "Episode 350, loss: 1.6888458967208861, total steps = 23.0\n",
      "Episode 375, loss: 1.8304792046546936, total steps = 30.0\n",
      "Episode 400, loss: 3.0613433837890627, total steps = 21.0\n",
      "Episode 425, loss: 4.065414834022522, total steps = 27.0\n",
      "Episode 450, loss: 6.379046773910522, total steps = 26.0\n",
      "Episode 475, loss: 8.687868356704712, total steps = 26.0\n",
      "Episode 500, loss: 13.334581756591797, total steps = 34.0\n",
      "Episode 525, loss: 15.765323257446289, total steps = 41.0\n",
      "Episode 550, loss: 22.20936450958252, total steps = 37.0\n",
      "Episode 575, loss: 26.377456283569337, total steps = 45.0\n",
      "Episode 600, loss: 26.424468994140625, total steps = 89.0\n",
      "Episode 625, loss: 26.690824508666992, total steps = 104.0\n",
      "Episode 650, loss: 23.227539443969725, total steps = 141.0\n",
      "Episode 675, loss: 22.23933334350586, total steps = 64.0\n",
      "Episode 700, loss: 21.755362129211427, total steps = 95.0\n",
      "Episode 725, loss: 22.790011978149415, total steps = 35.0\n",
      "Episode 750, loss: 25.38004913330078, total steps = 62.0\n",
      "Episode 775, loss: 32.943661308288576, total steps = 35.0\n",
      "Episode 800, loss: 46.79405689239502, total steps = 32.0\n",
      "Episode 825, loss: 32.35027561187744, total steps = 32.0\n",
      "Episode 850, loss: 16787.21552734375, total steps = 51.0\n",
      "Episode 875, loss: 40.54848918914795, total steps = 42.0\n",
      "Episode 900, loss: 20847.92802734375, total steps = 26.0\n",
      "Episode 925, loss: 63.24531860351563, total steps = 55.0\n",
      "Episode 950, loss: 2987.512072753906, total steps = 103.0\n",
      "Episode 975, loss: 21296.56943359375, total steps = 107.0\n",
      "Wall time: 33min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in range(episodes):\n",
    "    cumulative_loss = 0\n",
    "    if e < explore_steps: # pure exploration\n",
    "        epsilon = 1\n",
    "    else: # epsilon-greedy exploration/exploitation\n",
    "        epsilon = max(np.exp((e-explore_steps)/-50),0.1)\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        curr_state = observation\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            curr_state_reshape = np.reshape(curr_state,(1,4))\n",
    "            q_est = Q_estimator.predict(sess,curr_state_reshape)\n",
    "            action = np.argmin(q_est)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_state = observation\n",
    "        Buffer.add_new(curr_state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    for i in range(iterations):\n",
    "        states, actions, rewards, next_states = zip(*Buffer.batch(batch_size))\n",
    "        targets = []\n",
    "        for reward, next_state in zip(rewards,next_states):\n",
    "            next_state_reshape = np.reshape(next_state, (1,4))\n",
    "            q_max = np.max(Q_target.predict(sess,next_state_reshape))\n",
    "            target = reward + discount*q_max\n",
    "            targets.append(target)\n",
    "        state_array = np.stack(states,axis=0)\n",
    "        action_array = np.array(actions)\n",
    "        target_array = np.array(targets)\n",
    "        loss = Q_estimator.update(sess, state_array,action_array,target_array)\n",
    "        cumulative_loss += loss\n",
    "    if e%25 == 0:\n",
    "        Freezer.copy_and_freeze(sess)\n",
    "        print('Episode {}, loss: {}, total steps = {}'.format(e,cumulative_loss/iterations,total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luckiest of 100 random agents survives 55 time steps\n",
      "  Average random agent survived 20.76 time steps\n",
      "Best learned agent survives 140 time steps\n",
      "  Average learned agent survives 120.71 time steps\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random_results = []\n",
    "agent_results = []\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    random_results.append(total_steps)    \n",
    "\n",
    "for t in range(100):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    total_steps = 0\n",
    "    while not done:\n",
    "        state_reshape = np.reshape(observation,(1,4))\n",
    "        q_est = Q_estimator.predict(sess,state_reshape)\n",
    "        action = np.argmin(q_est)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    agent_results.append(total_steps)\n",
    "print('Luckiest of 100 random agents survives {} time steps'.format(max(random_results)))\n",
    "print('  Average random agent survived {} time steps'.format(np.mean(random_results)))\n",
    "print('Best learned agent survives {} time steps'.format(max(agent_results)))\n",
    "print('  Average learned agent survives {} time steps'.format(np.mean(agent_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (airl)",
   "language": "python",
   "name": "airl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
